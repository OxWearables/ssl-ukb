"""
Perform step inference with pretrained model on a UKB accelerometer file.
Requires a pretrained walking detector (RF and HMM) and peak counter (generated by train.py).

Arguments:
    Input file, in the format {eid}_*.cwa.gz

Example usage:
    python stepInference.py /data/ukb-accelerometer/group1/4027057_90001_0_0.cwa.gz

Output:
    Prediction DataFrame in {eid}.parquet format, stored in output_path/ (see conf/config.yaml).
    If the input file is stored in a groupX folder, output will be in output_path/groupX/

    An {eid}_info.csv file will be saved alongside the parquet file with the actipy info dict.
    An {eid}_summary.csv file in the same folder as the {eid}.parquet file
"""

import actipy
import argparse
import json
import os
import torch
import pandas as pd
import numpy as np
import time
import scipy.stats as stats
from pandas.tseries.frequencies import to_offset

from pathlib import Path
from omegaconf import OmegaConf
from datetime import datetime

from models.stepcount import StepCounter
import utils.utils as utils

DEVICE_HZ = 30  # Hz
WINDOW_SEC = 10  # seconds
WINDOW_OVERLAP_SEC = 0  # seconds
WINDOW_LEN = int(DEVICE_HZ * WINDOW_SEC)  # device ticks
WINDOW_OVERLAP_LEN = int(DEVICE_HZ * WINDOW_OVERLAP_SEC)  # device ticks
WINDOW_STEP_LEN = WINDOW_LEN - WINDOW_OVERLAP_LEN  # device ticks
TRAINED_MODELS = 5

log = utils.get_logger()

start_time = datetime.now()


def read(filepath, resample_hz='uniform'):

    p = Path(filepath)
    ftype = p.suffixes[0].upper()
    fsize = round(p.stat().st_size / (1024 * 1024), 1)

    if ftype in (".CSV", ".PKL"):

        if ftype == ".CSV":
            data = pd.read_csv(
                filepath,
                usecols=['time', 'x', 'y', 'z'],
                parse_dates=['time'],
                index_col='time'
            )
        elif ftype == ".PKL":
            data = pd.read_pickle(filepath)
        else:
            raise ValueError(f"Unknown file format: {ftype}")

        freq = infer_freq(data.index)
        sample_rate = int(np.round(pd.Timedelta('1s') / freq))

        data, info = actipy.process(
            data, sample_rate,
            lowpass_hz=None,
            calibrate_gravity=True,
            detect_nonwear=True,
            resample_hz=resample_hz,
        )

        info = {
            **{"Filename": filepath,
                "Device": ftype,
                "Filesize(MB)": fsize,
                "SampleRate": sample_rate},
            **info
        }

    elif ftype in (".CWA", ".GT3X", ".BIN"):

        data, info = actipy.read_device(
            filepath,
            lowpass_hz=None,
            calibrate_gravity=True,
            detect_nonwear=True,
            resample_hz=resample_hz,
        )

    return data, info


def vectorized_stride_v2(acc, time, window_size, stride_size):
    """
    Numpy vectorised windowing with stride (super fast!). Will discard the last window.

    :param np.ndarray acc: Accelerometer data array, shape (nsamples, nchannels)
    :param np.ndarray time: Time array, shape (nsamples, )
    :param int window_size: Window size in n samples
    :param int stride_size: Stride size in n samples
    :return: Windowed data and time arrays
    :rtype: (np.ndarray, np.ndarray)
    """
    start = 0
    max_time = len(time)

    sub_windows = (start +
                   np.expand_dims(np.arange(window_size), 0) +
                   # Create a rightmost vector as [0, V, 2V, ...].
                   np.expand_dims(np.arange(max_time + 1, step=stride_size), 0).T
                   )[:-1]  # drop the last one

    return acc[sub_windows], time[sub_windows]


def df_to_windows(df):
    """
    Convert a time series dataframe (e.g.: from actipy) to a windowed Numpy array.

    :param pd.DataFrame df: A dataframe with DatetimeIndex and x, y, z columns
    :return: Data array with shape (nwindows, WINDOW_LEN, 3), Time array with shape (nwindows, )
    :rtype: (np.ndarray, np.ndarray)
    """

    acc = df[['x', 'y', 'z']].to_numpy()
    time = df.index.to_numpy()

    # convert to windows
    x, t = vectorized_stride_v2(acc, time, WINDOW_LEN, WINDOW_STEP_LEN)

    # drop the whole window if it contains a NaN
    na = np.isnan(x).any(axis=1).any(axis=1)
    x = x[~na]
    t = t[~na]

    return x, t[:, 0]  # only return the first timestamp for each window


def resolve_path(path):
    """ Return parent folder, file name and file extension """
    p = Path(path)
    extension = p.suffixes[0]
    filename = p.name.rsplit(extension)[0]
    dirname = p.parent
    return dirname, filename, extension


def summarize(Y, adjust_estimates=False):

    if adjust_estimates:
        Y = impute_missing(Y)
        skipna = False
    else:
        # crude summary ignores missing data
        skipna = True

    def _sum(x):
        x = x.to_numpy()
        if skipna:
            return np.nansum(x)
        return np.sum(x)

    # there's a bug with .resample().sum(skipna)
    # https://github.com/pandas-dev/pandas/issues/29382

    total = np.round(Y.agg(_sum))  # total steps
    hourly = Y.resample('H').agg(_sum).round()  # steps, hourly
    daily = Y.resample('D').agg(_sum).round()  # steps, daily
    total_walk = (  # total walk (mins)
        (pd.Timedelta(infer_freq(Y.index)) * Y.mask(~Y.isna(), Y > 0).agg(_sum))
        .total_seconds() / 60
    )

    total = nanint(total)
    hourly = pd.to_numeric(hourly, downcast='integer')
    daily = pd.to_numeric(daily, downcast='integer')
    total_walk = float(total_walk)

    return {
        'total': total,
        'hourly': hourly,
        'daily': daily,
        'total_walk': total_walk,
    }


def impute_missing(data: pd.DataFrame, extrapolate=True):
    if extrapolate:
        # padding at the boundaries to have full 24h
        data = data.reindex(
            pd.date_range(
                data.index[0].floor('D'),
                data.index[-1].ceil('D'),
                freq=to_offset(infer_freq(data.index)),
                inclusive='left',
                name='time',
            ),
            method='nearest',
            tolerance=pd.Timedelta('1m'),
            limit=1)

    def fillna(subframe):
        if isinstance(subframe, pd.Series):
            x = subframe.to_numpy()
            nan = np.isnan(x)
            nanlen = len(x[nan])
            if 0 < nanlen < len(x):  # check x contains a NaN and is not all NaN
                x[nan] = np.nanmean(x)
                return x  # will be cast back to a Series automatically
            else:
                return subframe

    data = (
        data
        # first attempt imputation using same day of week
        .groupby([data.index.weekday, data.index.hour, data.index.minute])
        .transform(fillna)
        # then try within weekday/weekend
        .groupby([data.index.weekday >= 5, data.index.hour, data.index.minute])
        .transform(fillna)
        # finally, use all other days
        .groupby([data.index.hour, data.index.minute])
        .transform(fillna)
    )

    return data 


def nanint(x):
    if np.isnan(x):
        return x
    return int(x)


def infer_freq(x):
    """ Like pd.infer_freq but more forgiving """ 
    freq, _ = stats.mode(np.diff(x), keepdims=False)
    freq = pd.Timedelta(freq)
    return freq


class NpEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        if isinstance(obj, np.floating):
            return float(obj)
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        return json.JSONEncoder.default(self, obj)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(prog='Step Count', usage='Calculate the step count on a accelerometry cwa file.')
    parser.add_argument('input_file', type=str, help='input cwa file')
    parser.add_argument("--outdir", "-o", help="Enter folder location to save output files", default="outputs/")
    parser.add_argument('--model_type', '-m', help='Enter custom model file to use', default='rf')
    args = parser.parse_args()

    # Timing
    start = time.time()

    input_file = args.input_file

    # Output paths
    basename = resolve_path(input_file)[1]
    outdir = os.path.join(args.outdir, basename)
    os.makedirs(outdir, exist_ok=True)

    model_type = args.model_type

    # get pid and group from input string
    pid = basename.split('_')[0]
    group =  Path(input_file).parent.stem if 'group' in Path(input_file).parent.stem else None

    log.info(input_file)
    log.info('%s %s', group, pid)

    np.random.seed(42)
    torch.manual_seed(42)

    # load config
    cfg = OmegaConf.load("conf/config.yaml")

    GPU = cfg.gpu
    if GPU != -1:
        my_device = "cuda:" + str(GPU)
    else: 
        my_device = "cpu"

    # load data and construct dataloader
    data, info = read(input_file, cfg.data.sample_rate)
    log.info(data.head(1))
    log.info(info)
    info = pd.DataFrame(info, index=[1])


    # prepare dataset
    log.info('Windowing')
    X, T = df_to_windows(data)
    del data  # free up memory

    # Run model
    print("Running step counter...")
    n_iter = cfg.training.num_folds if (model_type == 'ssl') and cfg.training.num_folds else 1
    y = np.mean([StepCounter(cfg.peak_counter["weights_{}".format(model_type)].format(model_type, i),
                             cfg[model_type].weights.format(i),
                             model_type,
                             cfg.hmm["weights_{}".format(model_type)].format(i),
                             cfg.data.winsec,
                             cfg.data.sample_rate,
                             batch_size = cfg.ssl.batch_size, 
                             num_workers = cfg.num_workers, 
                             device = my_device,
                             ssl_repo_path = cfg.ssl_repo_path).predict(X, T) for i in range(n_iter)], axis=0, dtype=np.int64)

    y = pd.Series(y, index=T)

    # Save raw output timeseries
    y.to_parquet(f"{outdir}/{basename}_Steps.parquet", engine='pyarrow')

    # Summary
    summary = summarize(y)
    summary['pid'] = pid
    summary['hourly'].to_csv(f"{outdir}/{basename}_HourlySteps.csv")
    summary['daily'].to_csv(f"{outdir}/{basename}_DailySteps.csv")
    info['TotalSteps'] = summary['total']
    info['TotalWalking(min)'] = summary['total_walk']

    # Impute missing periods & recalculate summary
    summary_adj = summarize(y, adjust_estimates=True)
    summary_adj['hourly'].to_csv(f"{outdir}/{basename}_HourlyStepsAdjusted.csv")
    summary_adj['daily'].to_csv(f"{outdir}/{basename}_DailyStepsAdjusted.csv")
    info['TotalStepsAdjusted'] = summary_adj['total']
    info['TotalWalkingAdjusted(min)'] = summary_adj['total_walk']
    
    # Print
    print("\nSummary\n-------")
    print(json.dumps(info, indent=4, cls=NpEncoder))
    print("\nEstimated Daily Steps\n---------------------")
    print(pd.concat([
        summary['daily'].rename('Crude'), 
        summary_adj['daily'].rename('Adjusted')
    ], axis=1))

    # Timing
    end = time.time()
    print(f"Done! ({round(end - start,2)}s)")