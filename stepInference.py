"""
Perform step inference with pretrained model on a UKB accelerometer file.
Requires a pretrained walking detector (RF and HMM) and peak counter (generated by train.py).

Arguments:
    Input file, in the format {eid}_*.cwa.gz

Example usage:
    python stepInference.py /data/ukb-accelerometer/group1/4027057_90001_0_0.cwa.gz

Output:
    Prediction DataFrame in {eid}.parquet format, stored in output_path/ (see conf/config.yaml).
    If the input file is stored in a groupX folder, output will be in output_path/groupX/

    An {eid}_info.csv file will be saved alongside the parquet file with the actipy info dict.
    An {eid}_summary.csv file in the same folder as the {eid}.parquet file
"""

import actipy
import argparse
import json
import os
import torch
import pandas as pd
import numpy as np
import scipy.stats as stats
from pandas.tseries.frequencies import to_offset

from pathlib import Path
from omegaconf import OmegaConf
from datetime import datetime

from models.stepcount import StepCounter
import utils.utils as utils
from utils.summarisation import imputeMissing

DEVICE_HZ = 30  # Hz
WINDOW_SEC = 10  # seconds
WINDOW_OVERLAP_SEC = 0  # seconds
WINDOW_LEN = int(DEVICE_HZ * WINDOW_SEC)  # device ticks
WINDOW_OVERLAP_LEN = int(DEVICE_HZ * WINDOW_OVERLAP_SEC)  # device ticks
WINDOW_STEP_LEN = WINDOW_LEN - WINDOW_OVERLAP_LEN  # device ticks
TRAINED_MODELS = 5

log = utils.get_logger()

start_time = datetime.now()


def read(filepath, resample_hz='uniform'):

    p = Path(filepath)
    ftype = p.suffixes[0].upper()
    fsize = round(p.stat().st_size / (1024 * 1024), 1)

    if ftype in (".CSV", ".PKL"):

        if ftype == ".CSV":
            data = pd.read_csv(
                filepath,
                usecols=['time', 'x', 'y', 'z'],
                parse_dates=['time'],
                index_col='time'
            )
        elif ftype == ".PKL":
            data = pd.read_pickle(filepath)
        else:
            raise ValueError(f"Unknown file format: {ftype}")

        freq = infer_freq(data.index)
        sample_rate = int(np.round(pd.Timedelta('1s') / freq))

        data, info = actipy.process(
            data, sample_rate,
            lowpass_hz=None,
            calibrate_gravity=True,
            detect_nonwear=True,
            resample_hz=resample_hz,
        )

        info = {
            **{"Filename": filepath,
                "Device": ftype,
                "Filesize(MB)": fsize,
                "SampleRate": sample_rate},
            **info
        }

    elif ftype in (".CWA", ".GT3X", ".BIN"):

        data, info = actipy.read_device(
            filepath,
            lowpass_hz=None,
            calibrate_gravity=True,
            detect_nonwear=True,
            resample_hz=resample_hz,
        )

    return data, info


def vectorized_stride_v2(acc, time, window_size, stride_size):
    """
    Numpy vectorised windowing with stride (super fast!). Will discard the last window.

    :param np.ndarray acc: Accelerometer data array, shape (nsamples, nchannels)
    :param np.ndarray time: Time array, shape (nsamples, )
    :param int window_size: Window size in n samples
    :param int stride_size: Stride size in n samples
    :return: Windowed data and time arrays
    :rtype: (np.ndarray, np.ndarray)
    """
    start = 0
    max_time = len(time)

    sub_windows = (start +
                   np.expand_dims(np.arange(window_size), 0) +
                   # Create a rightmost vector as [0, V, 2V, ...].
                   np.expand_dims(np.arange(max_time + 1, step=stride_size), 0).T
                   )[:-1]  # drop the last one

    return acc[sub_windows], time[sub_windows]


def df_to_windows(df):
    """
    Convert a time series dataframe (e.g.: from actipy) to a windowed Numpy array.

    :param pd.DataFrame df: A dataframe with DatetimeIndex and x, y, z columns
    :return: Data array with shape (nwindows, WINDOW_LEN, 3), Time array with shape (nwindows, )
    :rtype: (np.ndarray, np.ndarray)
    """

    acc = df[['x', 'y', 'z']].to_numpy()
    time = df.index.to_numpy()

    # convert to windows
    x, t = vectorized_stride_v2(acc, time, WINDOW_LEN, WINDOW_STEP_LEN)

    # drop the whole window if it contains a NaN
    na = np.isnan(x).any(axis=1).any(axis=1)
    x = x[~na]
    t = t[~na]

    return x, t[:, 0]  # only return the first timestamp for each window


def summarize(Y, summary, impute=False):
    name = Y.name

    if impute:
        Y = imputeMissing(Y)
        suffix = f'{name}-imputed'
    else:
        suffix = name

    total = np.round(Y.agg(np.nansum))  # total steps
    hourly = Y.resample('H').agg(np.nansum).round()  # steps, hourly
    daily = Y.resample('D').agg(np.nansum).round()  # steps, daily
    total_walk = (  # total walk (mins)
        (pd.Timedelta(infer_freq(Y.index)) * Y.mask(~Y.isna(), Y > 0).agg(np.nansum))
        .total_seconds() / 60
    )

    total = nanint(total)
    hourly = pd.to_numeric(hourly, downcast='integer')
    daily = pd.to_numeric(daily, downcast='integer')
    total_walk = float(total_walk)

    summary['steps_total_{suffix}'] = int(total)

    if impute and len(daily) >= 8:
        daily_median = int(np.nanmedian(daily.iloc[0:7]))
        daily_mean = int(np.nanmean(daily.iloc[0:7]))
    else:
        daily_median = int(np.nanmedian(daily))
        daily_mean = int(np.nanmean(daily))

    summary[f'steps_daily_median_{suffix}'] = daily_median
    summary[f'steps_daily_mean_{suffix}'] = daily_mean
    summary[f'total_walk_min_{suffix}'] = int(total_walk)

    hourly.name = suffix
    daily.name = suffix

    return hourly, daily


def nanint(x):
    if np.isnan(x):
        return x
    return int(x)


def infer_freq(x):
    """ Like pd.infer_freq but more forgiving """
    freq, _ = stats.mode(np.diff(x), keepdims=False)
    freq = pd.Timedelta(freq)
    return freq


class NpEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        if isinstance(obj, np.floating):
            return float(obj)
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        return json.JSONEncoder.default(self, obj)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(prog='Step Count', usage='Calculate the step count on a accelerometry cwa file.')
    parser.add_argument('input_file', type=str, help='input cwa file')
    parser.add_argument("--outdir", "-o", help="Enter folder location to save output files", default="outputs/")
    parser.add_argument('--model_type', '-m', help='Enter model type to run',
                        choices=['rf', 'ssl', 'both'], default='both')
    args = parser.parse_args()

    input_file = args.input_file

    # Output paths
    basename = Path(input_file).stem
    outdir = args.outdir

    model_type = args.model_type

    if model_type == 'both':
        model_type = ['ssl', 'rf']
    else:
        model_type = [model_type]

    # get pid and group from input string
    pid = basename.split('_')[0]
    group = Path(input_file).parent.stem if 'group' in Path(input_file).parent.stem else None

    if group:
        outdir = os.path.join(outdir, group)

    os.makedirs(outdir, exist_ok=True)
    log.info(input_file)
    log.info('%s %s', group, pid)

    np.random.seed(42)
    torch.manual_seed(42)

    # load config
    cfg = OmegaConf.load("conf/config.yaml")

    GPU = cfg.gpu
    if GPU != -1:
        my_device = "cuda:" + str(GPU)
    else: 
        my_device = "cpu"

    # load data and construct dataloader
    data, info = read(input_file, cfg.data.sample_rate)
    log.info(data.head(1))
    log.info(info)
    info = pd.DataFrame(info, index=[0])

    # prepare dataset
    log.info('Windowing')
    X, T = df_to_windows(data)
    data_start = data.index[0]
    data_end = data.index[-1]
    del data

    # Run model
    log.info("Running step counter...")
    # n_iter = cfg.training.num_folds if (model_type == 'ssl') and cfg.training.num_folds else 1
    # y = np.mean([StepCounter(cfg.peak_counter["weights_{}".format(model_type)].format(model_type, i),
    #                          cfg[model_type].weights.format(i),
    #                          model_type,
    #                          cfg.hmm["weights_{}".format(model_type)].format(i),
    #                          cfg.data.winsec,
    #                          cfg.data.sample_rate,
    #                          batch_size=cfg.ssl.batch_size,
    #                          num_workers=cfg.num_workers,
    #                          device=my_device,
    #                          ssl_repo_path=cfg.ssl_repo_path).predict(X, T) for i in range(n_iter)], axis=0, dtype=np.int64)

    i = 0  # hardcode model 0
    y = {}
    for mtype in model_type:
        log.info(f'Running {mtype}')
        y[mtype] = StepCounter(cfg.peak_counter["weights_{}".format(mtype)].format(mtype, i),
                               cfg[mtype].weights.format(i),
                               mtype,
                               cfg.hmm["weights_{}".format(mtype)].format(i),
                               cfg.data.winsec,
                               cfg.data.sample_rate,
                               batch_size=cfg.ssl.batch_size,
                               num_workers=cfg.num_workers,
                               device=my_device,
                               ssl_repo_path=cfg.ssl_repo_path).predict(X, T)

    df_y = pd.concat([pd.Series(v, index=T, dtype=int, name=k) for k, v in y.items()], axis=1)

    newindex = pd.date_range(data_start, data_end, freq='{s}S'.format(s=WINDOW_SEC))
    df_y = df_y.reindex(newindex, method='nearest', fill_value=np.nan, tolerance='20S')

    # Save raw output timeseries
    df_y.to_parquet(f"{outdir}/{pid}_steps.parquet", engine='pyarrow')

    df_y['missing'] = df_y.isna().any(axis=1)

    wear_time = info['WearTime(days)'].round(2)
    good_coverage = not (df_y['missing']  # check there's at least some data for each hour pocket
                         .groupby(df_y['missing'].index.hour)
                         .all().any())
    good_weartime = wear_time >= 3  # check there's at least 3 days of wear time

    # Summary
    summary = {
        'eid': pid,
        'StartTime': info['StartTime'],
        'EndTime': info['EndTime'],
        'WearTime(days)': wear_time,
        'CalibrationOK': info['CalibOK'],
        'quality-goodWearTime': int(good_coverage and good_weartime)
    }

    hourly, daily = [], []  # contains lists of pd.Series
    for mtype in model_type:
        h, d = summarize(df_y[mtype], summary)
        h_imp, d_imp = summarize(df_y[mtype], summary, impute=True)
        hourly.extend([h, h_imp])
        daily.extend([d, d_imp])

    summary = pd.DataFrame(summary, index=[0])
    summary.to_csv(f"{outdir}/{pid}_summary.csv", index=False)

    # convert individual Series to DataFrame
    df_hourly = pd.concat(hourly, axis=1)
    df_daily = pd.concat(daily, axis=1)

    df_hourly.to_csv(f"{outdir}/{pid}_steps_hourly.csv", index_label='timestamp')
    df_daily.to_csv(f"{outdir}/{pid}_steps_daily.csv", index_label='timestamp')

    # Timing
    end_time = datetime.now()
    log.info(f'Duration: {end_time - start_time}')
